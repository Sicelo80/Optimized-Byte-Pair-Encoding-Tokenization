{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8094848,"sourceType":"datasetVersion","datasetId":4779402}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tokenization for South African Languages\n\nThis notebook demonstrates the process of tokenizing text data from various South African languages using a language-optimized Byte Pair Encoding (BPE) tokenizer.\n\n## Setup and Imports\n\nWe start by importing the necessary libraries and setting up the environment.","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nfrom tokenizers import Tokenizer, trainers, pre_tokenizers, decoders\nfrom tokenizers.models import BPE\nfrom tqdm import tqdm\nfrom collections import Counter","metadata":{"execution":{"iopub.status.busy":"2024-07-31T09:18:35.586428Z","iopub.execute_input":"2024-07-31T09:18:35.586899Z","iopub.status.idle":"2024-07-31T09:18:35.594007Z","shell.execute_reply.started":"2024-07-31T09:18:35.586812Z","shell.execute_reply":"2024-07-31T09:18:35.592604Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Load and process text data\n","metadata":{}},{"cell_type":"markdown","source":"*Load text data and prepend language-specific tokens.\n\n    Args:\n        data_paths (dict): Dictionary with data file paths for each language and split.\n        language_tokens (dict): Mapping of languages to language-specific tokens.\n        max_train_sequences (int, optional): Maximum number of sequences for training. Defaults to None.\n        max_val_sequences (int, optional): Maximum number of sequences for validation. Defaults to None.\n        max_test_sequences (int, optional): Maximum number of sequences for testing. Defaults to None.\n\n    Returns:\n        tuple: Dictionaries containing loaded text data for each split (train, validation, test).*","metadata":{}},{"cell_type":"code","source":"def read_texts(data_paths, language_tokens, max_train_sequences=None, max_val_sequences=None, max_test_sequences=None):\n\n    train_texts, val_texts, test_texts = {}, {}, {}\n\n    for split, split_paths in data_paths.items():\n        split_texts = {}\n        for language, file_path in split_paths.items():\n            language_token = language_tokens[language]\n            path = Path(file_path)\n            with path.open(\"r\", encoding=\"utf-8\") as f:\n                lines = f.readlines()\n                if split == \"train\":\n                    split_lines = lines[:max_train_sequences] if max_train_sequences else lines\n                elif split == \"validation\":\n                    split_lines = lines[:max_val_sequences] if max_val_sequences else lines\n                elif split == \"test\":\n                    split_lines = lines[:max_test_sequences] if max_test_sequences else lines\n\n                split_texts[language] = [language_token + \" \" + line.strip() for line in split_lines]\n\n        if split == \"train\":\n            train_texts = split_texts\n        elif split == \"validation\":\n            val_texts = split_texts\n        elif split == \"test\":\n            test_texts = split_texts\n\n    return train_texts, val_texts, test_texts","metadata":{"execution":{"iopub.status.busy":"2024-07-31T09:18:51.366541Z","iopub.execute_input":"2024-07-31T09:18:51.367866Z","iopub.status.idle":"2024-07-31T09:18:51.379623Z","shell.execute_reply.started":"2024-07-31T09:18:51.367825Z","shell.execute_reply":"2024-07-31T09:18:51.378030Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"*We define the paths for the training, validation, and test datasets and specify the language tokens.\n\n*","metadata":{}},{"cell_type":"code","source":"language_tokens = {\n        \"sesotho\": \"<st>\",\n        \"setswana\": \"<tn>\",\n        \"xhosa\": \"<xh>\",\n        \"xitsonga\": \"<ts>\",\n        \"zulu\": \"<zu>\"\n    }\n    \ndata_files = {\n        \"train\": {\n            \"sesotho\": \"/kaggle/input/tokenization/South_African_Languages_Preprocessed_Datasets/South_African_Languages_Preprocessed_Datasets/sesotho_train.txt\",\n            \"setswana\": \"/kaggle/input/tokenization/South_African_Languages_Preprocessed_Datasets/South_African_Languages_Preprocessed_Datasets/setswana_train.txt\",\n            \"xhosa\": \"/kaggle/input/tokenization/South_African_Languages_Preprocessed_Datasets/South_African_Languages_Preprocessed_Datasets/xhosa_train.txt\",\n            \"xitsonga\": \"/kaggle/input/tokenization/South_African_Languages_Preprocessed_Datasets/South_African_Languages_Preprocessed_Datasets/xitsonga_train.txt\",\n            \"zulu\": \"/kaggle/input/tokenization/South_African_Languages_Preprocessed_Datasets/South_African_Languages_Preprocessed_Datasets/zulu_train.txt\",\n        },\n        \"validation\": {\n            \"sesotho\": \"/kaggle/input/tokenization/South_African_Languages_Preprocessed_Datasets/South_African_Languages_Preprocessed_Datasets/sesotho_validation.txt\",\n            \"setswana\": \"/kaggle/input/tokenization/South_African_Languages_Preprocessed_Datasets/South_African_Languages_Preprocessed_Datasets/setswana_validation.txt\",\n            \"xhosa\": \"/kaggle/input/tokenization/South_African_Languages_Preprocessed_Datasets/South_African_Languages_Preprocessed_Datasets/xhosa_validation.txt\",\n            \"xitsonga\": \"/kaggle/input/tokenization/South_African_Languages_Preprocessed_Datasets/South_African_Languages_Preprocessed_Datasets/xitsonga_validation.txt\",\n            \"zulu\": \"/kaggle/input/tokenization/South_African_Languages_Preprocessed_Datasets/South_African_Languages_Preprocessed_Datasets/zulu_validation.txt\",\n        },\n        \"test\": {\n            \"sesotho\": \"/kaggle/input/tokenization/South_African_Languages_Preprocessed_Datasets/South_African_Languages_Preprocessed_Datasets/sesotho_test.txt\",\n            \"setswana\": \"/kaggle/input/tokenization/South_African_Languages_Preprocessed_Datasets/South_African_Languages_Preprocessed_Datasets/setswana_test.txt\",\n            \"xhosa\": \"/kaggle/input/tokenization/South_African_Languages_Preprocessed_Datasets/South_African_Languages_Preprocessed_Datasets/xhosa_test.txt\",\n            \"xitsonga\": \"/kaggle/input/tokenization/South_African_Languages_Preprocessed_Datasets/South_African_Languages_Preprocessed_Datasets/xitsonga_test.txt\",\n            \"zulu\": \"/kaggle/input/tokenization/South_African_Languages_Preprocessed_Datasets/South_African_Languages_Preprocessed_Datasets/zulu_test.txt\",\n        },\n    }\n    \ntrain_sequences = 100000\nvalidation_sequences = 1000\ntest_sequences = 1000\n    \ntrain_texts, validation_texts, test_texts = read_texts(\ndata_files, \nlanguage_tokens, \nmax_train_sequences=train_sequences, \nmax_val_sequences=validation_sequences, \nmax_test_sequences=test_sequences\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-31T09:21:56.852759Z","iopub.execute_input":"2024-07-31T09:21:56.853208Z","iopub.status.idle":"2024-07-31T09:21:57.489912Z","shell.execute_reply.started":"2024-07-31T09:21:56.853166Z","shell.execute_reply":"2024-07-31T09:21:57.488215Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m validation_sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     35\u001b[0m test_sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m---> 37\u001b[0m train_texts, validation_texts, test_texts \u001b[38;5;241m=\u001b[39m \u001b[43mread_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43mlanguage_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43mmax_train_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43mmax_val_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43mmax_test_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_sequences\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[3], line 10\u001b[0m, in \u001b[0;36mread_texts\u001b[0;34m(data_paths, language_tokens, max_train_sequences, max_val_sequences, max_test_sequences)\u001b[0m\n\u001b[1;32m      8\u001b[0m language_token \u001b[38;5;241m=\u001b[39m language_tokens[language]\n\u001b[1;32m      9\u001b[0m path \u001b[38;5;241m=\u001b[39m Path(file_path)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     11\u001b[0m     lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/pathlib.py:1119\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1118\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/tokenization/South_African_Languages_Preprocessed_Datasets/South_African_Languages_Preprocessed_Datasets/sesotho_train.txt'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/tokenization/South_African_Languages_Preprocessed_Datasets/South_African_Languages_Preprocessed_Datasets/sesotho_train.txt'","output_type":"error"}]},{"cell_type":"markdown","source":"# Identify common words in the corpus","metadata":{}},{"cell_type":"markdown","source":"* Identify common words in the training corpus, filtering out language-specific tokens\n    and ensuring a minimum word length.\n\n    Args:\n        train_texts (dict): Dictionary of training texts for each language.\n        num_common_words (int): Number of common words to identify.\n        language_tokens (list): List of language-specific tokens to exclude.\n        min_word_length (int, optional): Minimum number of characters a word must contain. Defaults to 2.\n\n    Returns:\n        list: List of common words.*","metadata":{}},{"cell_type":"code","source":"def identify_common_words(train_texts, num_common_words, language_tokens, min_word_length):\n    counter = Counter()\n    for texts in train_texts.values():\n        for text in texts:\n            counter.update(text.split())\n\n    # Filter out language-specific tokens and short words\n    common_words = [word for word, _ in counter.most_common(num_common_words)\n                    if word not in language_tokens and len(word) >= min_word_length]\n    return common_words","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_of_common_words = 50 #Number of common words \nmin_word_length = 4  # Minimum number of characters a word must contain\n\n# Identify common words\ncommon_words = identify_common_words(train_texts, num_of_common_words, language_tokens, min_word_length)\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initialize and configure the tokenizer with common words","metadata":{}},{"cell_type":"markdown","source":"*Initialize and configure the tokenizer with special tokens and common words.\n\n    Args:\n        special_tokens (list): List of special tokens to add to the tokenizer.\n        common_words (list): List of common words to initialize the vocabulary with.\n\n    Returns:\n        Tokenizer: Configured tokenizer instance.*","metadata":{}},{"cell_type":"code","source":"def initialize_tokenizer(special_tokens, common_words):\n    tokenizer = Tokenizer(BPE())\n    # Add special tokens first\n    tokenizer.add_special_tokens(special_tokens)\n    # Add common words to the tokenizer\n    tokenizer.add_tokens(common_words)\n    tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n    tokenizer.decoder = decoders.WordPiece()\n    return tokenizer\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"special_tokens = list(language_tokens.values()) + [\"<unk>\", \"<pad>\", \"<mask>\", \"<s>\", \"</s>\", \"[UNK]\"]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Tokenizer","metadata":{}},{"cell_type":"markdown","source":"*Train the tokenizer with the provided training texts.\n\n    Args:\n        tokenizer (Tokenizer): The tokenizer to be trained.\n        train_texts (dict): Dictionary of training texts for each language.\n        special_tokens (list): List of special tokens.\n        max_vocab_size (int): Maximum vocabulary size.\n        min_frequency (int): Minimum frequency for BPE merges.\n        num_epochs (int, optional): Number of epochs for training. Defaults to 1.*","metadata":{}},{"cell_type":"code","source":"def train_tokenizer(tokenizer, train_texts, special_tokens, max_vocab_size, min_frequency, num_epochs=1):\n    trainer = trainers.BpeTrainer(\n        special_tokens=special_tokens,\n        vocab_size=max_vocab_size,\n        min_frequency=min_frequency,\n        show_progress=True,\n    )\n\n    with tqdm(total=num_epochs, desc=\"Training\", unit=\"epoch\") as pbar:\n        for _ in range(num_epochs):\n            iterator = (text for lang_texts in train_texts.values() for text in lang_texts)\n            tokenizer.train_from_iterator(iterator, trainer=trainer)\n            pbar.update(1)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = 1000\nmerging_threshold = 2\ntrain_tokenizer(BPE_Optimized_Tokenizer, train_texts, special_tokens, max_vocab_size=vocab_size, min_frequency=merging_threshold)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save the trained tokenizer","metadata":{}},{"cell_type":"markdown","source":"*Save the trained tokenizer to the specified path.\n\n    Args:\n        tokenizer (Tokenizer): The tokenizer to save.\n        path (str): The path to save the tokenizer to.*","metadata":{}},{"cell_type":"code","source":"\ndef save_tokenizer(tokenizer, path):\n    tokenizer.save(path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" save_dir = Path(\"/kaggle/working/\")\n    save_dir.mkdir(parents=True, exist_ok=True)\n    BPE_Optimized_Tokenizer.save(str(save_dir / \"Tokenizer(Language_Optimized)\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding a Text","metadata":{}},{"cell_type":"markdown","source":"*We demonstrate the tokenization process with an example text.\n\n*","metadata":{}},{"cell_type":"code","source":"text = \"Sanibonani Emakhaya.\"\ntokenized_text = BPE_Optimized_Tokenizer.encode(text).tokens\nprint(\"Original sequence:\", text)\nprint(\"Tokenized sequence:\", tokenized_text)","metadata":{},"execution_count":null,"outputs":[]}]}